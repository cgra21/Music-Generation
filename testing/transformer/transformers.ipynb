{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689eae8f-740a-468a-aab6-0ab57ea31c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e43ed66-59b2-4719-b80f-7046102d21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for every model\n",
    "# this class keeps track of other sub modules, i.e conv layers\n",
    "# or transformer layers\n",
    "# as well as the loss and forward pass layers\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, \n",
    "                 d_hid: int, nlayers: int, droupout: float = 0.5):\n",
    "        # ntokens - number of unique tokens (including special tokens such as padding or eos\n",
    "        #\n",
    "        # d_model - input vector dimension, i.e. the dimension the vocabulary will be mapped to\n",
    "        # this is what is learned when training, either through masking or seq-to-seq or some other task\n",
    "        #\n",
    "        # nhead - number of attention heads, this takes an input in three parameters\n",
    "        # Query, Key and Value, each token has 3 vectors in each of these vector spaces.\n",
    "        # These are computed by passing the intial seq embedding through 3 seperate linear layers, on for Q, K and V respectively\n",
    "        # where each linear layer is of size Emb x Emb\n",
    "        # the self attention mechanism then calculates scores by taking the dot product of the Query, and Key vectors\n",
    "        # This results in a vector of size |T| (# of tokens in a given input)\n",
    "        # A softmax function is applied, this determines the weight of each tokens value vector\n",
    "        # The weighted sum of the value vectors forms the output for each input token\n",
    "        # In multi-headed attention, these matricies are split across each attention head\n",
    "        # Then an attention score is computed for each head\n",
    "        # More details can be found: https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\n",
    "        #\n",
    "        # d_hid - dimension of hidden layers, i.e. the feed forward net after the attention step\n",
    "        # n_layers - # of encoding layers\n",
    "        # dropout - rate of dropout applied to the output of each sublayer\n",
    "        \n",
    "        super().__init__() # needed for multi-class inheritance\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, droupout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "        # This is used to transform the output of the model into a vector of size n_token, which is\n",
    "        # the probability of the next token in the sequence\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        def init_weights(self) -> None:\n",
    "            initrange = 0.1\n",
    "            # Intialize embedding with uniform weights form -0.1 to 0.1\n",
    "            # .data allows access to the underlying tensor of the nn.Embedding class\n",
    "            # .uniform_ will modify the weights in place\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "            # set linear bias to zero in-place\n",
    "            self.linear.bias.zero_()\n",
    "            # intialize linear weights in-place\n",
    "            self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "            \n",
    "        # This function defines the forward pass of the model,\n",
    "        # that is, what steps it will take when data is input\n",
    "        def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "            \"\"\" Arguments:\n",
    "                src: Tensor, shape ``[seq_len, batch_size]``\n",
    "                src_mask: Tensor, shape ``[seq_len, seq_len]`` - Used to mask so future tokens do not have influence in prediction\n",
    "\n",
    "            Returns:\n",
    "                output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "            \"\"\"\n",
    "            \n",
    "            # This is a heuristic defined in th original \"Attention is all you need\"\n",
    "            # The embeddings typically have a variance of 1/d\n",
    "            # This step will increase the variance to ~1\n",
    "            # This will reduce the probability of a vanishing gradient\n",
    "            src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "            # positional encoding\n",
    "            src = self.pos_encoder(src)\n",
    "            \n",
    "            # Now pass through our nlayer encoder\n",
    "            output = self.transformer_encoder(src, src_mask)\n",
    "            output = self.linear(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9865089-c873-4b08-bb4c-7895e0d3356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(device)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096ca376-c009-4c9b-ae61-ac6e6f3254e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have inheritance, we can change the super class and inherit the changes down the line\n",
    "# This is what I will need to edit to get the same encoding that is used in \n",
    "# \"musicautobot\"\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # intialize dropout layer\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # torch.arange(max_len) - produce 1D tensor that contains 0 to max_len -1\n",
    "        # unsqueeze(1) will expand this to a 2D tensor of size [max_len, 1]\n",
    "        # so the resulting tensor will look like: [[0], [1], ..., [max_len -1]]\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        # torch.arange(0, d_model, 2) - 1D tensor from 0 to d_model by steps of 2\n",
    "        # math.log(10000.0) is a constant chosen for the original paper in order to create a series that decrease exponentially when multiplie by positoinal values\n",
    "        # torch.exp() - apply exp to each value of the tensor\n",
    "        # result = exp(-(2i/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Intialize positional encoding\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        # Apply positoinal encoding to each position\n",
    "        # Even positions\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        # Odd positions\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Create a buffer, that is a tensor that is part of a model but\n",
    "        # does not have gradients that are to be updated during training\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''Arguments:\n",
    "            x: Tensor, shape: [seq_len, batch_size, embedding_size]\n",
    "        '''\n",
    "        # Add x to its computed positional encoding for a given sequence length\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3b3fab-fe3d-47e8-b3f7-14cc6116e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58bf6bb-d72f-4ab3-b3a3-6a318a418da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials = ['unk'])\n",
    "vocab.set_default_index(vocab['unk'])\n",
    "\n",
    "\n",
    "# This funciton will tokenize and vocabularize the data in chunks\n",
    "# hence the type ShardFilterIterData - each shard is put into a tensor containing the vocab\n",
    "# Then, will remove all tensors with zero elements (t.numel() > 0\n",
    "# Then concat them into a singular tensor along the 0th dimension\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    '''Converts raw text into a flat Tensor.'''\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size) # Shape ''[seq_len, batch_size]''\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21538f78-5bfa-4856-83a6-c8b39c53ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d3ecc22-2466-436e-85a1-36a308a150f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cjgra\\anaconda3\\envs\\music_gen\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a86d52-73af-429f-8dbb-76b79042303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma = 0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) -1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        loss = criterion(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) -1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1700344e-72d6-4363-9922-5bb867bbed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 10.39 | loss  8.10 | ppl  3298.93\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch  7.16 | loss  6.85 | ppl   943.26\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch  6.40 | loss  6.42 | ppl   613.67\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch  6.29 | loss  6.30 | ppl   543.63\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch  6.36 | loss  6.19 | ppl   485.90\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch  6.31 | loss  6.15 | ppl   470.32\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch  6.88 | loss  6.12 | ppl   452.76\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch  7.23 | loss  6.11 | ppl   450.02\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch  6.66 | loss  6.03 | ppl   413.91\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch  6.41 | loss  6.02 | ppl   410.00\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch  6.41 | loss  5.90 | ppl   365.50\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch  7.03 | loss  5.97 | ppl   392.02\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch  6.45 | loss  5.95 | ppl   383.78\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  6.89 | loss  5.88 | ppl   357.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 21.82s | valid loss  5.86 | valid ppl   349.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch  6.42 | loss  5.86 | ppl   350.48\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch  6.42 | loss  5.85 | ppl   346.60\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch  6.58 | loss  5.67 | ppl   290.32\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch  6.58 | loss  5.71 | ppl   302.45\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch  6.43 | loss  5.65 | ppl   285.20\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch  6.52 | loss  5.69 | ppl   294.46\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch  6.47 | loss  5.69 | ppl   297.25\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch  6.56 | loss  5.72 | ppl   304.80\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch  6.52 | loss  5.65 | ppl   285.17\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch  6.65 | loss  5.67 | ppl   290.80\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch  6.97 | loss  5.55 | ppl   257.50\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch  6.42 | loss  5.65 | ppl   283.77\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch  6.55 | loss  5.64 | ppl   282.82\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch  6.82 | loss  5.58 | ppl   265.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 20.52s | valid loss  5.64 | valid ppl   281.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch  6.59 | loss  5.62 | ppl   274.66\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch  6.54 | loss  5.63 | ppl   278.10\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch  6.55 | loss  5.43 | ppl   227.03\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch  6.53 | loss  5.48 | ppl   239.98\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch  6.46 | loss  5.44 | ppl   230.61\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch  6.47 | loss  5.49 | ppl   241.05\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch  7.50 | loss  5.50 | ppl   245.59\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch  7.46 | loss  5.52 | ppl   250.54\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch  6.68 | loss  5.47 | ppl   238.22\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch  6.45 | loss  5.49 | ppl   242.34\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch  6.50 | loss  5.36 | ppl   213.48\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch  6.57 | loss  5.46 | ppl   235.57\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch  6.49 | loss  5.47 | ppl   238.26\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch  6.98 | loss  5.40 | ppl   221.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 21.10s | valid loss  5.55 | valid ppl   257.26\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac51b8-6665-4c31-9741-b325399a6388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6f809-03d2-43a6-ac0a-7586ec45ee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c73e14-00a3-44a5-a7b5-4460b1c017f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbae730f-cb1d-4e31-8c41-9e8895d96403",
   "metadata": {},
   "source": [
    "## Now that we understand the underlying workings of how a pytorch transformer works, we can expand the application to music\n",
    "The first step is modifying out MIDI files to be in a form that is able to be processed by the transformer model\n",
    "\n",
    "musicautobot did this with it's numpy encoder, but I may try a different way here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca697f1-d823-4c0d-971a-1f33f54fb25b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
