{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689eae8f-740a-468a-aab6-0ab57ea31c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e43ed66-59b2-4719-b80f-7046102d21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for every model\n",
    "# this class keeps track of other sub modules, i.e conv layers\n",
    "# or transformer layers\n",
    "# as well as the loss and forward pass layers\n",
    "class TransformerModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, \n",
    "                 d_hid: int, nlayers: int, droupout: float = 0.5):\n",
    "        # ntokens - number of unique tokens (including special tokens such as padding or eos\n",
    "        #\n",
    "        # d_model - input vector dimension, i.e. the dimension the vocabulary will be mapped to\n",
    "        # this is what is learned when training, either through masking or seq-to-seq or some other task\n",
    "        #\n",
    "        # nhead - number of attention heads, this takes an input in three parameters\n",
    "        # Query, Key and Value, each token has 3 vectors in each of these vector spaces.\n",
    "        # These are computed by passing the intial seq embedding through 3 seperate linear layers, on for Q, K and V respectively\n",
    "        # where each linear layer is of size Emb x Emb\n",
    "        # the self attention mechanism then calculates scores by taking the dot product of the Query, and Key vectors\n",
    "        # This results in a vector of size |T| (# of tokens in a given input)\n",
    "        # A softmax function is applied, this determines the weight of each tokens value vector\n",
    "        # The weighted sum of the value vectors forms the output for each input token\n",
    "        # In multi-headed attention, these matricies are split across each attention head\n",
    "        # Then an attention score is computed for each head\n",
    "        # More details can be found: https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853\n",
    "        #\n",
    "        # d_hid - dimension of hidden layers, i.e. the feed forward net after the attention step\n",
    "        # n_layers - # of encoding layers\n",
    "        # dropout - rate of dropout applied to the output of each sublayer\n",
    "        \n",
    "        super().__init__() # needed for multi-class inheritance\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, droupout)\n",
    "        self.transformer_encoder = TransformerEncoder(encode_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, n_token)\n",
    "        # This is used to transform the output of the model into a vector of size n_token, which is\n",
    "        # the probability of the next token in the sequence\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        def init_weights(self) -> None:\n",
    "            initrange = 0.1\n",
    "            # Intialize embedding with uniform weights form -0.1 to 0.1\n",
    "            # .data allows access to the underlying tensor of the nn.Embedding class\n",
    "            # .uniform_ will modify the weights in place\n",
    "            self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "            # set linear bias to zero in-place\n",
    "            self.linear.bias.zero_()\n",
    "            # intialize linear weights in-place\n",
    "            self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "            \n",
    "        # This function defines the forward pass of the model,\n",
    "        # that is, what steps it will take when data is input\n",
    "        def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "            \"\"\" Arguments:\n",
    "                src: Tensor, shape ``[seq_len, batch_size]``\n",
    "                src_mask: Tensor, shape ``[seq_len, seq_len]`` - Used to mask so future tokens do not have influence in prediction\n",
    "\n",
    "            Returns:\n",
    "                output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "            \"\"\"\n",
    "            \n",
    "            # This is a heuristic defined in th original \"Attention is all you need\"\n",
    "            # The embeddings typically have a variance of 1/d\n",
    "            # This step will increase the variance to ~1\n",
    "            # This will reduce the probability of a vanishing gradient\n",
    "            src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "            # positional encoding\n",
    "            src = self.pos_encoder(src)\n",
    "            \n",
    "            # Now pass through our nlayer encoder\n",
    "            output = self.transformer_encoder(src, src_mask)\n",
    "            output = self.linear(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096ca376-c009-4c9b-ae61-ac6e6f3254e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have inheritance, we can change the super class and inherit the changes down the line\n",
    "# This is what I will need to edit to get the same encoding that is used in \n",
    "# \"musicautobot\"\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # intialize dropout layer\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        \n",
    "        # torch.arange(max_len) - produce 1D tensor that contains 0 to max_len -1\n",
    "        # unsqueeze(1) will expand this to a 2D tensor of size [max_len, 1]\n",
    "        # so the resulting tensor will look like: [[0], [1], ..., [max_len -1]]\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "        # torch.arange(0, d_model, 2) - 1D tensor from 0 to d_model by steps of 2\n",
    "        # math.log(10000.0) is a constant chosen for the original paper in order to create a series that decrease exponentially when multiplie by positoinal values\n",
    "        # torch.exp() - apply exp to each value of the tensor\n",
    "        # result = exp(-(2i/d_model))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Intialize positional encoding\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        # Apply positoinal encoding to each position\n",
    "        # Even positions\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        # Odd positions\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Create a buffer, that is a tensor that is part of a model but\n",
    "        # does not have gradients that are to be updated during training\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        '''Arguments:\n",
    "            x: Tensor, shape: [seq_len, batch_size, embedding_size]\n",
    "        '''\n",
    "        # Add x to its computed positional encoding for a given sequence length\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b6c02-9afa-44d6-855d-d43e8d5d894a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_gen",
   "language": "python",
   "name": "music_gen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
